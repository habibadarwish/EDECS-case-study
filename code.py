# -*- coding: utf-8 -*-
"""Untitled45.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zwUqNfiMrvgZfOpIgwO0IzO-n8RXA6Bb
"""

!pip install pdfplumber

!pip install chromadb

!pip install langchain_community

!pip install langchain_huggingface

import os
import pdfplumber
from langchain_huggingface import HuggingFacePipeline
from langchain.vectorstores import Chroma
from langchain_huggingface import HuggingFaceEmbeddings
from langchain.text_splitter import CharacterTextSplitter
from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, pipeline

# Initialize LLM and embeddings
def initialize_tools():
    try:
        # Load local BART model for text2text-generation
        model_name = "facebook/bart-large"
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        model = AutoModelForSeq2SeqLM.from_pretrained(model_name)
        llm_pipeline = pipeline(
            "text2text-generation",
            model=model,
            tokenizer=tokenizer,
            max_length=512,
            temperature=0.7,
            device=-1  # CPU; use 0 for GPU if available
        )
        llm = HuggingFacePipeline(pipeline=llm_pipeline)
        embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
        return llm, embeddings
    except Exception as e:
        print(f"Error initializing tools: {e}")
        return None, None

def load_project_docs():
    dummy_docs = [
        "Project ID: PRJ-001, Concrete spec: Grade 30, Delivery: 2025-08-01",
        "Project ID: PRJ-002, Steel rebar: 16mm, Supplier: ABC Corp",
        "Project ID: PRJ-001, Safety protocol: OSHA-compliant, updated 2025-06-15"
    ]
    return dummy_docs

# Create vector store from project documents
def create_vector_store(docs, embeddings):
    try:
        text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
        split_docs = text_splitter.split_text("\n".join(docs))
        vectorstore = Chroma.from_texts(split_docs, embeddings, collection_name="project_docs")
        return vectorstore
    except Exception as e:
        print(f"Error creating vector store: {e}")
        return None

# Parse RFI PDF using pdfplumber
def parse_rfi(file_path):
    try:
        with pdfplumber.open(file_path) as pdf:
            text = "".join(page.extract_text() or "" for page in pdf.pages)
        if not text.strip():
            raise ValueError("No text extracted from PDF")
        return text
    except Exception as e:
        print(f"Error parsing RFI PDF: {e}")
        return None

# Extract key details from RFI using LLM
def extract_rfi_details(text, llm):
    try:
        prompt = f"""
        Extract the following from this RFI text:
        - Main question
        - Project ID
        - Urgency (High, Medium, Low)
        Text: {text}
        Return as a structured response: Question: ..., Project ID: ..., Urgency: ...
        """
        response = llm.invoke(prompt)
        return response
    except Exception as e:
        print(f"Error extracting RFI details: {e}")
        return None

# Query vector store for relevant project info
def query_project_docs(query, vectorstore):
    try:
        results = vectorstore.similarity_search(query, k=2)
        return "\n".join([doc.page_content for doc in results])
    except Exception as e:
        print(f"Error querying vector store: {e}")
        return ""

# Generate RFI response using LLM
def generate_response(details, project_info, llm):
    try:
        prompt = f"""
        Based on the RFI details: {details}
        And project information: {project_info}
        Draft a professional response to the RFI in 100-150 words.
        Use a polite and clear tone suitable for construction project communication.
        """
        response = llm.invoke(prompt)
        return response
    except Exception as e:
        print(f"Error generating response: {e}")
        return None

# Save response to file
def save_response(response, output_path="rfi_response.txt"):
    try:
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(response)
        print(f"Response saved to {output_path}")
    except Exception as e:
        print(f"Error saving response: {e}")

# Main function
def main():
    # Initialize tools
    llm, embeddings = initialize_tools()
    if not llm or not embeddings:
        print("Failed to initialize tools. Exiting.")
        return

    # Load and index project documents
    docs = load_project_docs()
    if not docs:
        print("No project documents loaded. Exiting.")
        return
    vectorstore = create_vector_store(docs, embeddings)
    if not vectorstore:
        print("Failed to create vector store. Exiting.")
        return

    # Process RFI
    rfi_file = "sample_rfi1.pdf"
    rfi_text = parse_rfi(rfi_file)
    if not rfi_text:
        print("Failed to parse RFI. Exiting.")
        return

    details = extract_rfi_details(rfi_text, llm)
    if not details:
        print("Failed to extract RFI details. Exiting.")
        return

    project_info = query_project_docs(details, vectorstore)
    if not project_info:
        print("No relevant project info found. Proceeding with response generation.")

    # Generate and save response
    response = generate_response(details, project_info, llm)
    if response:
        save_response(response)
    else:
        print("Failed to generate response.")

if __name__ == "__main__":
    main()